{"cells":[{"cell_type":"code","source":["spark_df = sqlContext.read.format(\"csv\").load(\"/FileStore/tables/cal_housing.data\", header=True, inferSchema=True)\nspark_df.show(2)\nhousing_df =  sqlContext.read.format(\"csv\").load(\"FileStore/tables/cal_housing.domain\", header=True, inferSchema=True)\nhousing_df.show(2)"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["housing_df.collect()"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["# Split lines on commas\nspark_df.first()\n# Inspect the first 2 lines \nspark_df.take(2)"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["# Import the necessary modules \nfrom pyspark.sql import Row\n\n# Map the RDD to a DF\ndf = spark_df.rdd.map(lambda line: Row(longitude=line[0], \n                              latitude=line[1], \n                              housingMedianAge=line[2],\n                              totalRooms=line[3],\n                              totalBedRooms=line[4],\n                              population=line[5], \n                              households=line[6],\n                              medianIncome=line[7],\n                              medianHouseValue=line[8])).toDF()"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["df.show(20)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["from pyspark.sql.types import *\n\ndf = df.withColumn(\"longitude\", df[\"longitude\"].cast(FloatType()))  .withColumn(\"latitude\", df[\"latitude\"].cast(FloatType())).withColumn(\"housingMedianAge\",df[\"housingMedianAge\"].cast(FloatType())).withColumn(\"totalRooms\", df[\"totalRooms\"].cast(FloatType())).withColumn(\"totalBedRooms\", df[\"totalBedRooms\"].cast(FloatType())).withColumn(\"households\", df[\"households\"].cast(FloatType())).withColumn(\"medianIncome\", df[\"medianIncome\"].cast(FloatType())).withColumn(\"medianHouseValue\", df[\"medianHouseValue\"].cast(FloatType()))\n     "],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["# Import all from `sql.types`\nfrom pyspark.sql.types import *\n\n# Write a custom function to convert the data type of DataFrame columns\ndef convertColumn(df, names, newType):\n  for name in names: \n     df = df.withColumn(name, df[name].cast(newType))\n  return df \n\n# Assign all column names to `columns`\ncolumns = ['households', 'housingMedianAge', 'latitude', 'longitude', 'medianHouseValue', 'medianIncome', 'population', 'totalBedRooms', 'totalRooms']\n\n# Conver the `df` columns to `FloatType()`\ndf = convertColumn(df, columns, FloatType())\n"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["df.select('population','totalBedRooms').show(10)\n"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["df.groupBy(\"housingMedianAge\").count().sort(\"housingMedianAge\",ascending=False).show()\n"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["df.describe().show()\n"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["# Import all from `sql.functions` \nfrom pyspark.sql.functions import *\n\n# Adjust the values of `medianHouseValue`\ndf = df.withColumn(\"medianHouseValue\", col(\"medianHouseValue\")/100000)\n\n# Show the first 2 lines of `df`\ndf.take(2)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["# Import all from `sql.functions` if you haven't yet\nfrom pyspark.sql.functions import *\n\n# Divide `totalRooms` by `households`\nroomsPerHousehold = df.select(col(\"totalRooms\")/col(\"households\"))\n\n# Divide `population` by `households`\npopulationPerHousehold = df.select(col(\"population\")/col(\"households\"))\n\n# Divide `totalBedRooms` by `totalRooms`\nbedroomsPerRoom = df.select(col(\"totalBedRooms\")/col(\"totalRooms\"))\n# Add the new columns to `df`\ndf = df.withColumn(\"roomsPerHousehold\", col(\"totalRooms\")/col(\"households\")) \\\n   .withColumn(\"populationPerHousehold\", col(\"population\")/col(\"households\")) \\\n   .withColumn(\"bedroomsPerRoom\", col(\"totalBedRooms\")/col(\"totalRooms\"))\n   \n# Inspect the result\ndf.first()"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["# Re-order and select columns\ndf = df.select(\"medianHouseValue\", \n              \"totalBedRooms\", \n              \"population\", \n              \"households\", \n              \"medianIncome\", \n              \"roomsPerHousehold\", \n              \"populationPerHousehold\", \n              \"bedroomsPerRoom\")"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["# Import `DenseVector`\nfrom pyspark.ml.linalg import DenseVector\n\n# Define the `input_data` \ninput_data = df.rdd.map(lambda x: (x[0], DenseVector(x[1:])))\n# Replace `df` with the new DataFrame\ndf = spark.createDataFrame(input_data, [\"label\", \"features\"])"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["# Import `StandardScaler` \nfrom pyspark.ml.feature import StandardScaler\n\n# Initialize the `standardScaler`\nstandardScaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\")\n# Fit the DataFrame to the scaler\nscaler = standardScaler.fit(df)\n\n# Transform the data in `df` with the scaler\nscaled_df = scaler.transform(df)\n\n# Inspect the result\nscaled_df.take(2)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["# Split the data into train and test sets\ntrain_data, test_data = scaled_df.randomSplit([.8,.2],seed=1234)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["# Import `LinearRegression`\nfrom pyspark.ml.regression import LinearRegression\n\n# Initialize `lr`\nlr = LinearRegression(labelCol=\"label\", maxIter=10, regParam=0.3, elasticNetParam=0.8)\n\n# Fit the data to the model\nlinearModel = lr.fit(train_data)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["# Generate predictions\npredicted = linearModel.transform(test_data)\n\n# Extract the predictions and the \"known\" correct labels\npredictions = predicted.select(\"prediction\").rdd.map(lambda x: x[0])\nlabels = predicted.select(\"label\").rdd.map(lambda x: x[0])\n# Zip `predictions` and `labels` into a list\npredictionAndLabel = predictions.zip(labels).collect()\n\n# Print out first 5 instances of `predictionAndLabel` \npredictionAndLabel[:5]"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["# Coefficients for the model\nlinearModel.coefficients\n\n# Intercept for the model\nlinearModel.intercept\n"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["# Get the RMSE\nlinearModel.summary.rootMeanSquaredError\n\n# Get the R2\nlinearModel.summary.r2"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":21}],"metadata":{"name":"california_housing dataset","notebookId":1722302363214497},"nbformat":4,"nbformat_minor":0}
